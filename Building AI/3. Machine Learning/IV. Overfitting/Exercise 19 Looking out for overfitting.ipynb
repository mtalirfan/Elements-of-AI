{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 19: Looking out for overfitting\n",
    "\n",
    "## Alrighty. We have now reached a point where we understand at least some machine learning techniques, and you're probably eager to try them out. Let's go and build some cool... ALARM ALARM ALARM! What? A warning siren starts blaring with a sign 'Fasten your overfitting safety belt'. What the heck is going on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginner\n",
    "\n",
    "Letâ€™s for a moment imagine you have a data set on 1000 email messages labeled as either spam or not. Out of the 1000 messages, 990 are legitimate emails, and 10 are spam.\n",
    "\n",
    "Then you split your data into training and test sets in such a way that both labels are present in both sets in equal ratios, and then train a classifier on the data.\n",
    "\n",
    "What would you set as the baseline accuracy that your model has to outperform in order to be considered worthwhile?\n",
    "\n",
    "Select the correct answer\n",
    "\n",
    "99% accuracy (classify everything as the majority class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate\n",
    "\n",
    "The program below uses the so called k-nearest neighbors algorithm. The idea is to not only look at the single nearest training data point (neighbor) but for example the five nearest points, if k=5. The normal nearest neighbor classifier amounts to using k=1.\n",
    "\n",
    "The program does the classification for some value of k and outputs an image that shows how the different things are classified. Modify the program so that the program prints out the training and testing accuracy and make it possible to use different values for k.\n",
    "\n",
    "Hint: You can get the model accuracy using function knn.score. For example: knn.score(x_train, y_train) returns the training set accuracy after you have first created the classifier by calling knn.fit(x_train, y_train).\n",
    "\n",
    "Try different values of k to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 1.000000\n",
      "testing accuracy: 0.860606\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# create fake data\n",
    "x, y = make_moons(\n",
    "    n_samples=500,  # the number of observations\n",
    "    random_state=42,\n",
    "    noise=0.3\n",
    ")\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Create a classifier and fit it to our data\n",
    "knn = KNeighborsClassifier(n_neighbors=42)\n",
    "knn.fit(x_train, y_train)\n",
    "train_acc = knn.score(x_train, y_train)\n",
    "\n",
    "print(\"training accuracy: %f\" % train_acc)\n",
    "test_acc = knn.score(x_test, y_test)\n",
    "print(\"testing accuracy: %f\" % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following values of k do you think was \"best\"?\n",
    "\n",
    "k = 42\n",
    "\n",
    "Why?\n",
    "\n",
    "it gave the highest testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "\n",
    "The program below uses the k-nearest neighbors algorithm. The idea is to not only look at the single nearest training data point (neighbor) but for example the five nearest points, if k=5. The normal nearest neighbor classifier amounts to using k=1.\n",
    "\n",
    "Write a program that does the classification for some value of k and prints out the training and testing accuracy.\n",
    "\n",
    "Hint: You can get the model accuracy for a given set using the function knn.score.\n",
    "\n",
    "Try different values of k to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.847761\n",
      "testing accuracy: 0.884848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# do not edit this\n",
    "# create fake data\n",
    "x, y = make_moons(\n",
    "    n_samples=500,  # the number of observations\n",
    "    random_state=42,\n",
    "    noise=0.3\n",
    ")\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Create a classifier and fit it to our data\n",
    "knn = KNeighborsClassifier(n_neighbors=133)\n",
    "knn.fit(x_train, y_train)\n",
    "train_acc = knn.score(x_train, y_train)\n",
    "\n",
    "print(\"training accuracy: %f\" % train_acc)\n",
    "test_acc = knn.score(x_test, y_test)\n",
    "print(\"testing accuracy: %f\" % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be a reasonable baseline accuracy your model should outperform in order for it to be considered useful?\n",
    "\n",
    "0.50\n",
    "\n",
    "There are two classes, and the data points are evenly split among them. Assigning every point to either class, or picking a class randomly would result in a 50% accuracy.\n",
    "\n",
    "Which of the following values of k do you think was \"best\"?\n",
    "\n",
    "k = 42\n",
    "\n",
    "Why?\n",
    "\n",
    "it gave the highest testing accuracy\n",
    "\n",
    "Is it possible to have a higher test set accuracy than training set accuracy?\n",
    "\n",
    "yes\n",
    "\n",
    "It is possible, and for many reasons. For example if you are doing a classification task like here, if your data sets have class imbalance it can easily lead to such a scenario, or if your test set points in this example here would have been picked far away from the decision boundary then they would have been easier to classify correctly than those near the border."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
