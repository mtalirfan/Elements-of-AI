{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 16: Nearest neighbor\n",
    "\n",
    "## Beginner\n",
    "\n",
    "In the above diagram, we show a collection of training data items, some of which belong to one class (purple / stars) and some to another class (blue / circles). In addition, there are two test data items, the letters (A and B), which we are going to classify using the nearest neighbor method. To which class do the two letters belong?\n",
    "\n",
    "If the classification is done based on the color of the single nearest neighbour:\n",
    "\n",
    "both are blue\n",
    "\n",
    "If the classification is done based on the color of the five nearest neighbour:\n",
    "\n",
    "A is blue, B is purple\n",
    "\n",
    "In general, not related to the image above, what happens if you determine the class of a test item by the class of K neighbours, where K is equal to the number of data points you have in your training set?\n",
    "\n",
    "You classify everything as what is the most prominent class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate\n",
    "\n",
    "The program below uses the library [sklearn](https://scikit-learn.org/) to generate a random dataset. You don't need to be familiar with sklearn, we explain all the necessary information below. Each sample in the dataset has two input features X and one binary output class y. We can think of a sample as a cabin, with its size and price as its input features, and whether we like it (1) or not (0) as its output class.\n",
    "\n",
    "The program's goal is to classify the cabins based on their nearest neighbor's class. That is, predict whether we would like a cabin based on our opinion of another cabin with the most similar input features.\n",
    "\n",
    "The program first generates the random dataset and splits it into training and test sets. Then, for each cabin in the test set, it identifies its nearest neighbor from the cabins in the train set using the distance function. However, the program has very high standards and dislikes all the cabins y_predict[i] = 0.\n",
    "\n",
    "Your goal is to make the program smarter by predicting the output class (y_predict) for each cabin in the test set based on the output class (y_train) of its nearest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# create random data with two classes\n",
    "X, y = make_blobs(n_samples=16, n_features=2, centers=2, center_box=(-2, 2))\n",
    "\n",
    "# scale the data so that all values are between 0.0 and 1.0\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# split two data points from the data as test data and\n",
    "# use the remaining n-2 points as the training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=2)\n",
    "\n",
    "# place-holder for the predicted classes\n",
    "y_predict = np.empty(len(y_test), dtype=np.int64)\n",
    "\n",
    "# produce line segments that connect the test data points\n",
    "# to the nearest neighbors for drawing the chart\n",
    "lines = []\n",
    "\n",
    "\n",
    "# distance function\n",
    "def dist(a, b):\n",
    "    sum = 0\n",
    "    for ai, bi in zip(a, b):\n",
    "        sum = sum + (ai - bi)**2\n",
    "    return np.sqrt(sum)\n",
    "\n",
    "\n",
    "def main(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    global y_predict\n",
    "    global lines\n",
    "\n",
    "    # process each of the test data points\n",
    "    for i, test_item in enumerate(X_test):\n",
    "        # calculate the distances to all training points\n",
    "        distances = [dist(train_item, test_item) for train_item in X_train]\n",
    "\n",
    "        # find the index of the nearest neighbor\n",
    "        nearest = np.argmin(distances)\n",
    "\n",
    "        # create a line connecting the points for the chart\n",
    "        lines.append(np.stack((test_item, X_train[nearest])))\n",
    "\n",
    "        # add your code here:\n",
    "        # y_predict[i] = 0          # this just classifies everything as 0\n",
    "        y_predict[i] = y_train[nearest]\n",
    "\n",
    "    print(y_predict)\n",
    "\n",
    "\n",
    "main(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "\n",
    "In the basic nearest neighbor classifier, the only thing that matters is the class label of the nearest neighbor. But the nearest neighbor may sometimes be noisy or otherwise misleading. Therefore, it may be better to also consider the other nearby data points in addition to the nearest neighbor.\n",
    "\n",
    "This idea leads us to the so called k-nearest neighbor method, where we consider all the k nearest neighbors. If k=3, for example, we'd take the three nearest points and choose the class label based on the majority class among them.\n",
    "\n",
    "The program below uses the library [sklearn](https://scikit-learn.org/) to generate a random dataset. You don't need to be familiar with sklearn, we explain all the necessary information below. Each sample in the dataset has two input features (X) and one binary output class (y). We can think of a sample as a cabin, with its size and price as its input features, and whether we like it (1) or not (0) as its output class.\n",
    "\n",
    "The program first generates the random dataset and splits it into training and test sets. Then, for each cabin in the test set, it identifies its nearest neighbor (k=1) from the cabins in the train set using the distance function. However, the program has very high standards and dislikes all the cabins y_predict[i] = 0.\n",
    "\n",
    "Your goal is to make the program smarter by predicting the output class (y_predict) for each cabin in the test set based on the majority output class (y_train) of its three nearest neighbor (k=3).\n",
    "\n",
    "Hint 1: After calculating all the distances in D (see the code below) you can use [np.argsort](https://stackoverflow.com/q/17901218) to sort the points in increasing distance.\n",
    "\n",
    "Hint 2: You can assume that there are only two possible class labels, 0 and 1. As this is the case, you can get the majority where class by np.round(np.mean(y)) where y is the list containing the class labels of the nearest neighbors. This works by calculating the mean of the class labels and rounding it to zero in case it is less than 0.5 and one otherwise, which is the same as the majority rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# create random data with two classes\n",
    "X, Y = make_blobs(n_samples=16, n_features=2, centers=2, center_box=(-2, 2))\n",
    "\n",
    "# scale the data so that all values are between 0.0 and 1.0\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# split two data points from the data as test data and\n",
    "# use the remaining n-2 points as the training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=2)\n",
    "\n",
    "# place-holder for the predicted classes\n",
    "y_predict = np.empty(len(y_test), dtype=np.int64)\n",
    "\n",
    "# produce line segments that connect the test data points\n",
    "# to the nearest neighbors for drawing the chart\n",
    "lines = []\n",
    "\n",
    "# distance function\n",
    "def dist(a, b):\n",
    "    sum = 0\n",
    "    for ai, bi in zip(a, b):\n",
    "        sum = sum + (ai - bi)**2\n",
    "    return np.sqrt(sum)\n",
    "\n",
    "\n",
    "def main(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    global y_predict\n",
    "    global lines\n",
    "\n",
    "    k = 3    # classify our test items based on the classes of 3 nearest neighbors\n",
    "\n",
    "    # process each of the test data points\n",
    "    for i, test_item in enumerate(X_test):\n",
    "        # calculate the distances to all training points\n",
    "        distances = [dist(train_item, test_item) for train_item in X_train]\n",
    "\n",
    "        # add your code here\n",
    "        nearest = np.argmin(distances)       # this just finds the nearest neighbour (so k=1)\n",
    "\n",
    "        nearest_indices = np.argsort(distances)\n",
    "        # print(nearest_indices)\n",
    "\n",
    "        nearest_labels = y_train[nearest_indices[:k]]\n",
    "        # print(nearest_labels)\n",
    "\n",
    "        # create a line connecting the points for the chart\n",
    "        # you may change this to do the same for all the k nearest neigbhors if you like\n",
    "        # but it will not be checked in the tests\n",
    "        lines.append(np.stack((test_item, X_train[nearest])))\n",
    "\n",
    "        y_predict[i] = np.round(np.mean(nearest_labels))\n",
    "\n",
    "    print(y_predict)\n",
    "\n",
    "main(X_train, X_test, y_train, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
