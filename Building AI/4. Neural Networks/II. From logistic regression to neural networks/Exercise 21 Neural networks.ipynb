{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 21: Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginner\n",
    "\n",
    "Consider the neural network in the picture above. How many input nodes does it have?\n",
    "\n",
    "3\n",
    "\n",
    "What about hidden nodes?\n",
    "\n",
    "4\n",
    "\n",
    "Letâ€™s focus on the hidden nodes. What is the value of the linear combination in node a_1 before an activation function is applied to it?\n",
    "\n",
    "3.848\n",
    "\n",
    "If we choose to use a linear activation function, what would be the output of this node?\n",
    "\n",
    "3.848\n",
    "\n",
    "If your final task would be to predict based on the height, weight, and length if a thing is a dog or a cat, what would you choose to be the activation function for the output layer?\n",
    "\n",
    "sigmoid, since the output will be a probability of it being one of them\n",
    "\n",
    "If we choose to use a sigmoid activation function, what would be the output of this node?\n",
    "\n",
    "~ 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate\n",
    "\n",
    "We have trained a simple neural network with a larger set of cabin price data. The network predicts the price of the cabin based on the attributes of the cabin. The network consists of an input layer with five nodes, a hidden layer with two nodes, a second hidden layer with two nodes, and finally an output layer with a single node. In addition, there is a single bias node for each hidden layer and the output layer.\n",
    "\n",
    "The program below uses the weights of this trained network to perform what is called a forward pass of the neural network. The forward pass is running the input variables through the neural network to obtain output, in this case the price of a cabin of given attributes.\n",
    "\n",
    "The program is incomplete though. The bias nodes are not used in the version below, and the activation functions for the hidden layers and the output layer have not been properly defined.\n",
    "\n",
    "Modify the program to use the bias nodes, and to use the ReLU activation function for the hidden nodes, and a linear (identity) activation for the output node. ReLU activation function returns either the input value of the function, or zero, whichever is the largest, and linear activation just returns the input as output. After these are done, get a prediction for the price of a cabin which is described by the following feature vector [74, 5, 10, 2, 100]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w0 = np.array([[ 1.19627687e+01,  2.60163283e-01],\n",
    "               [ 4.48832507e-01,  4.00666119e-01],\n",
    "                              [-2.75768443e-01,  3.43724167e-01],\n",
    "                   [ 2.29138536e+01,  3.91783025e-01],\n",
    "                   [-1.22397711e-02, -1.03029800e+00]])\n",
    "\n",
    "w1 = np.array([[11.5631751 , 11.87043684],\n",
    "                   [-0.85735419,  0.27114237]])\n",
    "\n",
    "w2 = np.array([[11.04122165],\n",
    "                   [10.44637262]])\n",
    "\n",
    "b0 = np.array([-4.21310294, -0.52664488])\n",
    "b1 = np.array([-4.84067881, -4.53335139])\n",
    "b2 = np.array([-7.52942418])\n",
    "\n",
    "x = np.array([[111, 13, 12, 1, 161],\n",
    "                 [125, 13, 66, 1, 468],\n",
    "                 [46, 6, 127, 2, 961],\n",
    "                 [80, 9, 80, 2, 816],\n",
    "                 [33, 10, 18, 2, 297],\n",
    "                 [85, 9, 111, 3, 601],\n",
    "                 [24, 10, 105, 2, 1072],\n",
    "                 [31, 4, 66, 1, 417],\n",
    "                 [56, 3, 60, 1, 36],\n",
    "                 [49, 3, 147, 2, 179]])\n",
    "y = np.array([335800., 379100., 118950., 247200., 107950., 266550.,  75850.,\n",
    "                93300., 170650., 149000.])\n",
    "\n",
    "\n",
    "def hidden_activation(z):\n",
    "    # ReLU activation. fix this!\n",
    "        return 0\n",
    "\n",
    "def output_activation(z):\n",
    "    # identity (linear) activation. fix this!\n",
    "        return 0\n",
    "\n",
    "x_test = [[72, 2, 25, 3, 450], [60, 3, 15, 1, 300], [74, 5, 10, 2, 100]]\n",
    "for item in x_test:\n",
    "    h1_in = np.dot(item, w0) # this calculates the linear combination of inputs and weights. it is missing the bias term, fix it!\n",
    "    h1_out = hidden_activation(h1_in) # apply activation function\n",
    "\n",
    "    h2_in = np.dot(h1_out, w1) # the output of the previous layer is the input for this layer. it is missing the bias term, fix it!\n",
    "    h2_out = hidden_activation(h2_in)\n",
    "\n",
    "    out_in = np.dot(h2_out, w2)\n",
    "    out = output_activation(out_in)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What price does the neural network predict for the cabin in question?\n",
    "\n",
    "roughly 233,000 euros\n",
    "\n",
    "What type of a machine learning problem is this?\n",
    "\n",
    "supervised learning\n",
    "\n",
    "How can we make sure we are not overfitting the neural network to the data?\n",
    "\n",
    "use cross-validation\n",
    "\n",
    "A simple way to keep overfitting is to use cross-validation. If you see that your testing error is huge compared to your training error, you are probably overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "\n",
    "We have trained a simple neural network with a larger set of cabin price data. The network predicts the price of the cabin based on the attributes of the cabin. The network consists of an input layer with five nodes, a hidden layer with two nodes, a second hidden layer with two nodes, and finally an output layer with a single node. In addition, there is a single bias node for each hidden layer and the output layer.\n",
    "\n",
    "The program below uses the weights of this trained network to perform what is called a forward pass of the neural network. The forward pass is running the input variables through the neural network to obtain output, in this case the price of a cabin of given attributes.\n",
    "\n",
    "The program is incomplete though. The program only does the forward pass up to the first hidden layer and is missing the second hidden layer and the output layer.\n",
    "\n",
    "Modify the program to do a full forward pass and print out the price prediction. To do this, write out the remaining forward pass operations and use the ReLU activation function for the hidden nodes, and a linear (identity) activation for the output node. ReLU activation function returns either the input value of the function, or zero, whichever is the largest, and linear activation just returns the input as output. After these are done, get a prediction for the price of a cabin which is described by the following feature vector [82, 2, 65, 3, 516]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "w0 = np.array([[ 1.19627687e+01,  2.60163283e-01],\n",
    "               [ 4.48832507e-01,  4.00666119e-01],\n",
    "                   [-2.75768443e-01,  3.43724167e-01],\n",
    "                   [ 2.29138536e+01,  3.91783025e-01],\n",
    "                   [-1.22397711e-02, -1.03029800e+00]])\n",
    "\n",
    "w1 = np.array([[11.5631751 , 11.87043684],\n",
    "                   [-0.85735419,  0.27114237]])\n",
    "\n",
    "w2 = np.array([[11.04122165],\n",
    "                   [10.44637262]])\n",
    "\n",
    "b0 = np.array([-4.21310294, -0.52664488])\n",
    "b1 = np.array([-4.84067881, -4.53335139])\n",
    "b2 = np.array([-7.52942418])\n",
    "\n",
    "x = np.array([[111, 13, 12, 1, 161],\n",
    "                 [125, 13, 66, 1, 468],\n",
    "                 [46, 6, 127, 2, 961],\n",
    "                 [80, 9, 80, 2, 816],\n",
    "                 [33, 10, 18, 2, 297],\n",
    "                 [85, 9, 111, 3, 601],\n",
    "                 [24, 10, 105, 2, 1072],\n",
    "                 [31, 4, 66, 1, 417],\n",
    "                 [56, 3, 60, 1, 36],\n",
    "                 [49, 3, 147, 2, 179]])\n",
    "y = np.array([335800., 379100., 118950., 247200., 107950., 266550.,  75850.,\n",
    "                93300., 170650., 149000.])\n",
    "\n",
    "\n",
    "def hidden_activation(z):\n",
    "    # ReLU activation. fix this!\n",
    "        return 0\n",
    "\n",
    "def output_activation(z):\n",
    "    # identity (linear) activation. fix this!\n",
    "        return 0\n",
    "\n",
    "x_test = [[82, 2, 65, 3, 516]]\n",
    "for item in x_test:\n",
    "    h1_in = np.dot(item, w0) + b0 # this calculates the linear combination of inputs and weights\n",
    "    h1_out = hidden_activation(h1_in) # apply activation function\n",
    "\n",
    "    # fill out the missing parts:\n",
    "    # the output of the first hidden layer, h1_out, will need to go through\n",
    "    # the second hidden layer with weights w1 and bias b1\n",
    "    # and finally to the output layer with weights w2 and bias b2.\n",
    "    # remember correct activations: relu in the hidden layers and linear (identity) in the output\n",
    "\n",
    "    print(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What price does the neural network predict for a cabin described by input vector [82, 2, 65, 3, 516] ?\n",
    "\n",
    "roughly 257,100 euros\n",
    "\n",
    "What type of machine learning is this?\n",
    "\n",
    "supervised learning\n",
    "\n",
    "How can we make sure we are not overfitting the neural network to the data?\n",
    "\n",
    "use cross-validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
